\section{Trabajos Relacionados} \label{sec:related_work_mask}

    En esta sección hacemos una revisión de los trabajos relacionados con los conceptos presentados en este capítulo. Si bien la literatura en tolerancia a fallas es extensa \cite{ShoomanBook, KrishnaBook, ReliabilityBook}, en general, las técnicas y metodologías son muchas veces descritas de una forma ad-hoc (dependiendo de cada problema), y carecen de un formalismo unificador que permita analizar diferentes problemas. Una clara excepción son los trabajos de Arora \cite{AroraGouda93} y Kulkarni \cite{DBLP:conf/ftrtft/KulkarniA00}, en donde, se definen las nociones de tolerancia a fallas enmascarante, no enmascarante y segura ante fallos. Estos también son estudiados de una manera formal en \cite{Avizienis2004}. Sin embargo, en estos trabajos fundacionales no se estudia métricas para estos conceptos, esta métricas son necesarias a la hora de diseñar mejores técnicas de tolerancia a fallas, o proveer técnicas nuevas.  Esta es la novedad que se introduce en esta tesis.

En los últimos años, se ha incrementado el interés en generalizaciones cuantitativas de la noción booleana de correctitud y sus interrogantes correspondientes en verificación cuantitativa \cite{BokerCHK14,CernyHR12,Henzinger10,Henzinger13}.
El \textit{framework} descrito en \cite{CernyHR12} es el trabajo más cercanamente relacionado a nuestro enfoque. 
Los autores generalizan la noción tradicional de relación de simulación a tres versiones diferentes de distancia de simulación: \emph{correctitud}, \emph{cobertura}, y \emph{robustez}.
Estas distancias se definen utilizando juegos cuantitativos con objetivos \emph{discounted-sum} 
y \emph{mean-payoff}, dos funciones de costo bien conocidas.
De forma similar a ese trabajo, también consideramos distancias entre sistemas puramente discretos (no probabilistas y sin considerar tiempo).

Las distancias de correctitud y cobertura se concentran en la parte nominal de los sistemas, y por lo tanto las fallas no cumplen un rol en estas distancias. Por otro lado, la distancia de robustez mide cuantos errores inesperados pueden ocurrir en la implementación de tal forma que el comportamiento resultante es tolerado por la especificación. Entonces, esta distancia puede ser utilizada para analizar la resiliencia de la implementación. Notemos que, la distancia de robustez solo puede ser aplicada a implementaciones correctas, es decir, implementaciones que preserven el comportamiento de la especificación pero tal vez no cubre todo su comportamiento. 
 Como ha sido notado en~\cite{CernyHR12}, la bisimilitud a veces implica una distancia de $1$. En este sentido, un grado mayor de robustez (como está definido en~\cite{CernyHR12}) se logra al recortar puntos críticos de la especificación. Además, los errores considerados en ese trabajo son transiciones que imitan a las originales pero con diferentes etiquetas. En contraste con esto, nuestro enfoque considera que las fallas son inyectadas en la implementación tolerante a fallas, donde sus comportamientos no son restringidos por el sistema nominal. Esto sigue la idea de la extensión de modelos en tolerancia a fallas donde se agrega comportamiento defectuoso al sistema nominal. Además, notemos que cuando no ocurren fallas, la distancia de enmascaramiento entre la especificación y la implementación es $0$ cuando son bisimilares, y es  $1$ en caso contrario.
Es útil destacar que la distancia de robustez de~\cite{CernyHR12} no es reflexiva. Creemos que todas estas definiciones de distancia entre sistemas capturan diferentes nociones útiles para el desarrollo de software, y pueden ser utilizadas en conjunto, de forma complementaria, para obtener una evaluación profunda de implementaciones tolerantes a fallas.